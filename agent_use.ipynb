{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35810c1c-c10e-4cef-b448-3d8507b5d6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Till Zemann\n",
    "# License: MIT License\n",
    "\n",
    "# https://gymnasium.farama.org/tutorials/training_agents/blackjack_tutorial/\n",
    "# https://gymnasium.farama.org/environments/toy_text/blackjack/\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.patches import Patch\n",
    "from tqdm import tqdm\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"Blackjack-v1\", sab=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9a49c0-23e6-44d6-899a-1f262f157954",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackjackAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float = 0.95,\n",
    "    ):\n",
    "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
    "        of state-action values (q_values), a learning rate and an epsilon.\n",
    "\n",
    "        Args:\n",
    "            learning_rate: The learning rate\n",
    "            initial_epsilon: The initial epsilon value\n",
    "            epsilon_decay: The decay for epsilon\n",
    "            final_epsilon: The final epsilon value\n",
    "            discount_factor: The discount factor for computing the Q-value\n",
    "        \"\"\"\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \"\"\"\n",
    "        Returns the best action with probability (1 - epsilon)\n",
    "        otherwise a random action with probability epsilon to ensure exploration.\n",
    "        \"\"\"\n",
    "        # with probability epsilon return a random action to explore the environment\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # with probability (1 - epsilon) act greedily (exploit)\n",
    "        else:\n",
    "            return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
    "        temporal_difference = (\n",
    "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
    "        )\n",
    "\n",
    "        self.q_values[obs][action] = (\n",
    "            self.q_values[obs][action] + self.lr * temporal_difference\n",
    "        )\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)\n",
    "\n",
    "    def save_state(self, path):\n",
    "        state = {\n",
    "            \"lr\": self.lr,\n",
    "            \"discount_factor\": self.discount_factor,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"epsilon_decay\": self.epsilon_decay,\n",
    "            \"final_epsilon\": self.final_epsilon,\n",
    "            \"training_error\": self.training_error,\n",
    "            \"q_values\": dict(self.q_values)\n",
    "        }\n",
    "        with open(path, 'wb') as file:\n",
    "            pickle.dump(state, file)\n",
    "        \n",
    "    def load_state(self, path):\n",
    "        with open(path, 'rb') as file:\n",
    "            state = pickle.load(file)\n",
    "            self.lr = state[\"lr\"]\n",
    "            self.epsilon = state[\"epsilon\"]\n",
    "            self.epsilon_decay = state[\"epsilon_decay\"]\n",
    "            self.final_epsilon = state[\"final_epsilon\"]\n",
    "            self.training_error = state[\"training_error\"]\n",
    "            self.lr = state[\"lr\"]\n",
    "            self.q_values = defaultdict(lambda: np.zeros(env.action_space.n), state[\"q_values\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d2afb2-7858-4ea5-8a61-640472e0802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "learning_rate = 0.01\n",
    "n_episodes = 100_000\n",
    "start_epsilon = 1.0\n",
    "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
    "final_epsilon = 0.1\n",
    "\n",
    "agent = BlackjackAgent(\n",
    "    learning_rate=learning_rate,\n",
    "    initial_epsilon=start_epsilon,\n",
    "    epsilon_decay=epsilon_decay,\n",
    "    final_epsilon=final_epsilon,\n",
    ")\n",
    "\n",
    "agent.load_state(\"state.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "601a9d11-6eb5-41b7-a6cc-66624f8897ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 42, 0: 8, -1: 50}\n"
     ]
    }
   ],
   "source": [
    "res = {\n",
    "    1: 0,\n",
    "    0: 0,\n",
    "    -1: 0\n",
    "    }\n",
    "\n",
    "for episode in range(100):\n",
    "    # print(\"-\" * 80)\n",
    "    obs, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.get_action(obs)\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "        agent.update(obs, action, reward, terminated, next_obs)\n",
    "        done = terminated or truncated\n",
    "        if done:\n",
    "            # print(reward)\n",
    "            res[reward] += 1\n",
    "        obs = next_obs\n",
    "\n",
    "    agent.decay_epsilon()\n",
    "    # print(\"-\" * 80)\n",
    "\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
